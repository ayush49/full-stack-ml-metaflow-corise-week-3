{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Full Stack Machine Learning's Week 3 Project!\n",
    "\n",
    "Welcome aboard the bustling streets of New York City, where every second counts and efficiency is paramount. As a key member of the data science team for YellowCab, your mission, should you choose to accept, is to leverage machine learning to predict taxi fares for rides throughout the city. With one of the most comprehensive datasets at your disposal, you'll explore the intriguing world of regression models in a real-world setting.\n",
    "\n",
    "## Your Journey with YellowCab\n",
    "\n",
    "YellowCab is a titan in NYC's transport industry, transporting millions of riders every year. You're at the forefront of enhancing their digital experience. By creating an accurate fare prediction model, you can transform YellowCab's operations, enhancing user experience by providing accurate fare estimates prior to booking.\n",
    "\n",
    "Remember, you're not just a data scientist - you're a trailblazer guiding YellowCab towards a data-centric future. By the end of this project, you'll have hands-on experience in building, evaluating, and deploying machine learning models for real-world applications. Are you ready to drive YellowCab's digital transformation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Diving into the Data - Exploratory Data Analysis\n",
    "\n",
    "Your first pit stop involves understanding your dataset - a rich collection of taxi trip records from yellow and green taxis, and For-Hire Vehicle (FHV) trips in NYC, accessible [here](https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet). \n",
    "\n",
    "Devote about an hour to acquaint yourself with the dataset's structure and quirks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from metaflow import S3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# configuration\n",
    "YELLOW = \"#FFBC00\"\n",
    "GREEN = \"#37795D\"\n",
    "PURPLE = \"#5460C0\"\n",
    "BACKGROUND = \"#F4EBE6\"\n",
    "colors = [GREEN, PURPLE]\n",
    "custom_params = {\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.facecolor\": BACKGROUND,\n",
    "    \"figure.facecolor\": BACKGROUND,\n",
    "    \"figure.figsize\": (8, 8),\n",
    "}\n",
    "sns_palette = sns.color_palette(colors, len(colors))\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a context manager to download data from the cloud.\n",
    "s3 = S3(s3root=\"s3://outerbounds-datasets/taxi\")\n",
    "\n",
    "# Use Metaflow S3 client to get the latest file.\n",
    "# This file is being updated every hour, simulating the changing, drifting, and sometimes broken nature of production data streams.\n",
    "obj = s3.get(\"latest.parquet\")\n",
    "# The goal is to write a flow that builds and cross-validates a model to predicts the total fare of each taxi rid (row) in the dataset.\n",
    "# Since the data is changing, Task 2 and 3 asks you to deploy a flow to production via Argo workflows, so your workflow can run automatically when this file changes in S3.\n",
    "\n",
    "# Load the contents of the parquet file in memory.\n",
    "df = pd.read_parquet(obj.path)\n",
    "\n",
    "s3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obviously_bad_data_filters = [\n",
    "    df.fare_amount > 0,  # fare_amount in US Dollars\n",
    "    df.trip_distance <= 100,  # trip_distance in miles\n",
    "    df.trip_distance > 0\n",
    "    # TODO: add some logic to filter out what you decide is bad data!\n",
    "    # TIP: Don't spend too much time on this step for this project though, it practice it is a never-ending process.\n",
    "]\n",
    "\n",
    "for f in obviously_bad_data_filters:\n",
    "    df = df[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.scatter(df.trip_distance, df.fare_amount, color=GREEN, alpha=0.3)\n",
    "ax.set_xlabel(\"Trip Distance\")\n",
    "ax.set_ylabel(\"Fare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Harnessing the Power of the Cloud - Argo Workflow Deployment\n",
    "With a solid understanding of your dataset, it's time to deploy a workflow to Argo Workflows. A key aspect of this task is the @trigger decorator. This decorator configures Argo to listen for updates to the latest.parquet file (mentioned in Task 1) and trigger the flow when an S3 event is emitted. This is a practical introduction to event-driven machine learning, a vital part of real-world ML systems.\n",
    "\n",
    "And here's the exciting part: This new capability to trigger workflows based on events has been recently introduced by Metaflow, marking a significant advancement in building scalable, real-world ML systems. Read more about this on [Metaflow's blog](https://outerbounds.com/blog/metaflow-event-triggering/).\n",
    "\n",
    "\n",
    "Notice the use of the `@trigger` flow-level decorator.\n",
    "In this flow, this is telling Argo to listen for events named `s3`, and to trigger a run of the flow when the S3 event is emitted. \n",
    "The S3 event your sandbox is listening for are the updates to the `latest.parquet` file mentioned in the comments in [Task 1](#task-1-eda). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/cloud/event_triggered_linear_regression.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/cloud/event_triggered_linear_regression.py\n",
    "from metaflow import FlowSpec, step, card, conda_base, current, Parameter, Flow, trigger\n",
    "from metaflow.cards import Markdown, Table, Image, Artifact\n",
    "\n",
    "URL = \"https://outerbounds-datasets.s3.us-west-2.amazonaws.com/taxi/latest.parquet\"\n",
    "DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "\n",
    "@trigger(events=[\"s3\"])\n",
    "@conda_base(\n",
    "    libraries={\n",
    "        \"pandas\": \"1.4.2\",\n",
    "        \"pyarrow\": \"11.0.0\",\n",
    "        \"numpy\": \"1.21.2\",\n",
    "        \"scikit-learn\": \"1.1.2\",\n",
    "    }\n",
    ")\n",
    "class TaxiFarePrediction(FlowSpec):\n",
    "    data_url = Parameter(\"data_url\", default=URL)\n",
    "\n",
    "    def transform_features(self, df):\n",
    "        # TODO:\n",
    "        # Try to complete tasks 2 and 3 with this function doing nothing like it currently is.\n",
    "        # Understand what is happening.\n",
    "        # Revisit task 1 and think about what might go in this function.\n",
    "\n",
    "        return df\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "        import pandas as pd\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        self.df = self.transform_features(pd.read_parquet(self.data_url))\n",
    "\n",
    "        # NOTE: we are split into training and validation set in the validation step which uses cross_val_score.\n",
    "        # This is a simple/naive way to do this, and is meant to keep this example simple, to focus learning on deploying Metaflow flows.\n",
    "        # In practice, you want split time series data in more sophisticated ways and run backtests.\n",
    "        self.X = self.df[\"trip_distance\"].values.reshape(-1, 1)\n",
    "        self.y = self.df[\"total_amount\"].values\n",
    "        self.next(self.linear_model)\n",
    "\n",
    "    @step\n",
    "    def linear_model(self):\n",
    "        \"Fit a single variable, linear model to the data.\"\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "\n",
    "        # TODO: Play around with the model if you are feeling it.\n",
    "        self.model = LinearRegression()\n",
    "\n",
    "        self.next(self.validate)\n",
    "\n",
    "    def gather_sibling_flow_run_results(self):\n",
    "        # storage to populate and feed to a Table in a Metaflow card\n",
    "        rows = []\n",
    "\n",
    "        # loop through runs of this flow\n",
    "        for run in Flow(self.__class__.__name__):\n",
    "            if run.id != current.run_id:\n",
    "                if run.successful:\n",
    "                    icon = \"✅\"\n",
    "                    msg = \"OK\"\n",
    "                    score = str(run.data.scores.mean())\n",
    "                else:\n",
    "                    icon = \"❌\"\n",
    "                    msg = \"Error\"\n",
    "                    score = \"NA\"\n",
    "                    for step in run:\n",
    "                        for task in step:\n",
    "                            if not task.successful:\n",
    "                                msg = task.stderr\n",
    "                row = [\n",
    "                    Markdown(icon),\n",
    "                    Artifact(run.id),\n",
    "                    Artifact(run.created_at.strftime(DATETIME_FORMAT)),\n",
    "                    Artifact(score),\n",
    "                    Markdown(msg),\n",
    "                ]\n",
    "                rows.append(row)\n",
    "            else:\n",
    "                rows.append(\n",
    "                    [\n",
    "                        Markdown(\"✅\"),\n",
    "                        Artifact(run.id),\n",
    "                        Artifact(run.created_at.strftime(DATETIME_FORMAT)),\n",
    "                        Artifact(str(self.scores.mean())),\n",
    "                        Markdown(\"This run...\"),\n",
    "                    ]\n",
    "                )\n",
    "        return rows\n",
    "\n",
    "    @card(type=\"corise\")\n",
    "    @step\n",
    "    def validate(self):\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "\n",
    "        self.scores = cross_val_score(self.model, self.X, self.y, cv=5)\n",
    "        current.card.append(Markdown(\"# Taxi Fare Prediction Results\"))\n",
    "        current.card.append(\n",
    "            Table(\n",
    "                self.gather_sibling_flow_run_results(),\n",
    "                headers=[\"Pass/fail\", \"Run ID\", \"Created At\", \"R^2 score\", \"Stderr\"],\n",
    "            )\n",
    "        )\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"Success!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TaxiFarePrediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../flows/cloud/event_triggered_linear_regression.py --environment=conda run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Taking Flight - Promoting Your Workflow to Production\n",
    "\n",
    "After thorough testing and validation, it's time to elevate your workflow to the production environment. Your model will then be running in the cloud, independent of manual triggers, serving predictions to improve YellowCab's service. To help you with this task, refer to [Metaflow's Argo documentation](https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the workflow to Argo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you run from this from the terminal you may need to adjust the path to the flow file, depending on where you saved it - what comes after %%writefile? \n",
    "! python ../flows/cloud/event_triggered_linear_regression.py --environment=conda --with retry argo-workflows create "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, go to the Argo deployment attached to your sandbox by finding your ID from the URL in your browser.\n",
    "\n",
    "Your sandbox has a unique ID, visible in the URL with this pattern.\n",
    "```\n",
    "https://vs-<YOUR ID>.outerbounds.dev/<DELETE EVERYTHING AFTER THIS>\n",
    "```\n",
    "\n",
    "Remove the `vs` part in front, and everything after `.dev`, then navigate to your sandboxes Argo deployment at:\n",
    "```\n",
    "https://argo-<YOUR ID>.outerbounds.dev/\n",
    "```\n",
    "\n",
    "Navigating to the \"Workflow Templates\" section of the Argo UI, you should find your workflow has been deployed:\n",
    "\n",
    "<img src=\"../img/argo-workflow-template.png\" width=\"1200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually trigger the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../flows/cloud/event_triggered_linear_regression.py --environment=conda argo-workflows trigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After triggering the worfklow, return to your Argo dashboard and navigate to the `Workflows` section to see your flow executing:\n",
    "\n",
    "<img src=\"../img/argo-workflow.png\" width=\"1200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is the flow \"automatically\" triggered? \n",
    "\n",
    "In the previous section, you manually triggered the deployed flow using a Metaflow command. \n",
    "So how will it run without that manual trigger in production? \n",
    "\n",
    "1. There is a script running in the background in your sandbox that looks at the `latest.parquet` file in S3 and emits an `S3` when it has changed since the last `TaxiFarePrediction` flow run was created. As mentioned earlier, the file is being updated every hour, time shifting the [NYC taxi data stream from January 2023](https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet).\n",
    "2. When the events are emitted, the same thing happens as the command you ran in [the previous section](#manually-trigger-the-workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Navigating Challenges - Handling Failures (Bonus Task)\n",
    "\n",
    "Real-world data is unpredictable. \n",
    "\n",
    "To prepare for this, we'll introduce a common scenario by intentionally corrupting some training data. Your task is to make your TaxiFarePrediction flow robust against such data quality issues.\n",
    "\n",
    "Can you,\n",
    "1. Figure out what is happening in the new data that is causing an error?\n",
    "2. Add a few lines of code to your `TaxiFarePrediction` flow's `start` step that will make your flow robust to this kind of data quality issue?\n",
    "3. How would you refactor the `TaxiFarePrediction` to use the decorators you learned in this week's lesson, such as `@catch`, `@retry`, etc.? \n",
    "\n",
    "(Optional) TODO: Write a report of your findings to these questions. \n",
    "\n",
    "(Optional) TODO: Update your production deployment of `TaxiFarePrediction` with the new approach(es)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
